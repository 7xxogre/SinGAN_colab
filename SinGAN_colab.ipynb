{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "SinGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "metadata": {
        "id": "pZI3JBnLHKdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5419cd-6674-4b72-cc2e-9c1aae8abb65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from datetime import datetime\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from glob import glob\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.utils.data as data\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import autograd\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch import nn\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.optim\r\n",
        "import torch.utils.data"
      ],
      "outputs": [],
      "metadata": {
        "id": "OpFjeQXkG2_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Before execute this notebook, create folder named \"SinGAN_data\" in your google drive and upload train image.\r\n",
        "data_dir = '/content/gdrive/MyDrive/SinGAN_data'\r\n",
        "\r\n",
        "# If you have pretrained model and you want to validate it, save your pretrained model's name to load_model.\r\n",
        "# And must valiation value change to 1\r\n",
        "validation = 0\r\n",
        "load_model = None\r\n",
        "\r\n",
        "# you can change gantype to \"wgangp\"\r\n",
        "gantype = 'zerogp'\r\n",
        "\r\n",
        "batch_size = 1\r\n",
        "gpu = 0\r\n",
        "img_size_min = 25\r\n",
        "# if you valiate pretrained model, must change img_size_max value to 1025\r\n",
        "img_size_max = 250"
      ],
      "outputs": [],
      "metadata": {
        "id": "MH-sa9lXH39v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if load_model is not None:\r\n",
        "    this_time_model = load_model\r\n",
        "else:\r\n",
        "    this_time_model = f'SinGAN_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_{gantype}'\r\n",
        "if os.path.isdir('./logs') is False:\r\n",
        "    os.makedirs('./logs')\r\n",
        "if os.path.isdir('./results') is False:\r\n",
        "    os.makedirs('./results')\r\n",
        "if load_model is None:\r\n",
        "    os.makedirs(os.path.join('./logs', this_time_model))      \r\n",
        "if os.path.isdir(os.path.join('./results', this_time_model)) is False:\r\n",
        "    os.makedirs(os.path.join('./results', this_time_model)) \r\n",
        "\r\n",
        "log_dir = os.path.join('./logs', this_time_model)\r\n",
        "res_dir = os.path.join('./results', this_time_model)"
      ],
      "outputs": [],
      "metadata": {
        "id": "nDkLV-HmHJMh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class SinDataset(data.Dataset):\r\n",
        "\r\n",
        "    def __init__(self, dir, transform):\r\n",
        "        self.data_dir = dir\r\n",
        "        self.transform = transform\r\n",
        "        self.image_dir = sorted(glob(os.path.join(self.data_dir, '*.jpg')))[0]\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.image_dir)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        with open(self.image_dir, 'rb') as f:\r\n",
        "            img = Image.open(f)\r\n",
        "            img = img.convert('RGB')\r\n",
        "            return self.transform(img)\r\n",
        "\r\n",
        "\r\n",
        "def get_dataset(data_dir):\r\n",
        "    # image processing\r\n",
        "    train_transforms = transforms.Compose([transforms.Resize((256,256)),\r\n",
        "                                           transforms.ToTensor(),\r\n",
        "                                           transforms.Normalize(mean=[0.5, 0.5, 0.5],\r\n",
        "                                                                std=[0.5, 0.5, 0.5])])\r\n",
        "    \r\n",
        "    val_transforms = transforms.Compose([transforms.Resize((256,256)), \r\n",
        "                                         transforms.ToTensor(), \r\n",
        "                                         transforms.Normalize(mean=[0.5, 0.5, 0.5],\r\n",
        "                                                              std=[0.5, 0.5, 0.5])])\r\n",
        "\r\n",
        "    train_dataset = SinDataset(data_dir, transform = train_transforms)\r\n",
        "    val_dataset = SinDataset(data_dir, transform = val_transforms)\r\n",
        "\r\n",
        "    return train_dataset, val_dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "aHLUS9XuIkOf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# datasets\r\n",
        "train_dataset, _ = get_dataset(data_dir)\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1,\r\n",
        "                                              shuffle = False, num_workers = 8,\r\n",
        "                                              pin_memory= True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QSImyTxJNVU",
        "outputId": "30b8173e-9171-44a9-958b-d0021b1df834"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "        self.hidden = 32\r\n",
        "        self.current_scale = 0\r\n",
        "\r\n",
        "        self.discriminators = nn.ModuleList()\r\n",
        "\r\n",
        "        temp_disc = nn.ModuleList()\r\n",
        "\r\n",
        "        temp_disc.append(nn.Sequential(nn.Conv2d(3, self.hidden, 3, 1, 1),\r\n",
        "                                       nn.LeakyReLU(0.2)))\r\n",
        "        for _ in range(3):\r\n",
        "            temp_disc.append(nn.Sequential(nn.Conv2d(self.hidden, self.hidden, 3, 1, 1),\r\n",
        "                                           nn.BatchNorm2d(self.hidden),\r\n",
        "                                           nn.LeakyReLU(0.2)))\r\n",
        "\r\n",
        "        temp_disc.append(nn.Sequential(nn.Conv2d(self.hidden, 1, 3, 1, 1)))\r\n",
        "        \r\n",
        "        temp_disc = nn.Sequential(*temp_disc)\r\n",
        "\r\n",
        "        self.discriminators.append(temp_disc)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.discriminators[self.current_scale](x)\r\n",
        "        return out\r\n",
        "\r\n",
        "    def progress(self):\r\n",
        "        self.current_scale += 1\r\n",
        "        if self.current_scale % 4 == 0:\r\n",
        "            self.hidden *= 2\r\n",
        "\r\n",
        "        temp_disc = nn.ModuleList()\r\n",
        "        temp_disc.append(nn.Sequential(nn.Conv2d(3, self.hidden, 3, 1, 1),\r\n",
        "                                       nn.LeakyReLU(0.2)))\r\n",
        "        for _ in range(3):\r\n",
        "            temp_disc.append(nn.Sequential(nn.Conv2d(self.hidden, self.hidden, 3, 1, 1),\r\n",
        "                                           nn.BatchNorm2d(self.hidden),\r\n",
        "                                           nn.LeakyReLU(0.2)))\r\n",
        "        temp_disc.append(nn.Sequential(nn.Conv2d(self.hidden, 1, 3, 1, 1)))\r\n",
        "        \r\n",
        "        temp_disc = nn.Sequential(*temp_disc)\r\n",
        "\r\n",
        "        if self.current_scale % 4 != 0:\r\n",
        "            # continue start learning from prev discriminator's parameters\r\n",
        "            temp_disc.load_state_dict(self.discriminators[-1].state_dict())\r\n",
        "\r\n",
        "        self.discriminators.append(temp_disc)\r\n",
        "        print(\"PROGRESSION DONE\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "S56ZMfGUKQGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class Generator(nn.Module):\r\n",
        "    def __init__(self, img_size_min, num_scale, scalefactor = 4/3):\r\n",
        "        super(Generator, self).__init__()\r\n",
        "        self.hidden = 32\r\n",
        "        self.current_scale = 0\r\n",
        "        self.img_size_min = img_size_min\r\n",
        "        self.scalefactor = scalefactor\r\n",
        "        self.num_scale = num_scale\r\n",
        "\r\n",
        "        self.size_list = [int(self.img_size_min * (self.scalefactor ** i)) for i in range(num_scale + 1)]\r\n",
        "        print(f\"size_list : {self.size_list}\")\r\n",
        "\r\n",
        "        self.generators = nn.ModuleList()\r\n",
        "\r\n",
        "        temp_gene = nn.ModuleList()\r\n",
        "\r\n",
        "        temp_gene.append(nn.Sequential(nn.Conv2d(3, self.hidden, 3, 1),\r\n",
        "                                             nn.BatchNorm2d(self.hidden),\r\n",
        "                                             nn.LeakyReLU(0.2)))\r\n",
        "        for _ in range(3):\r\n",
        "            temp_gene.append(nn.Sequential(nn.Conv2d(self.hidden, self.hidden, 3, 1),\r\n",
        "                                                 nn.BatchNorm2d(self.hidden),\r\n",
        "                                                 nn.LeakyReLU(0.2)))\r\n",
        "        temp_gene.append(nn.Sequential(nn.Conv2d(self.hidden, 3, 3, 1),\r\n",
        "                                             nn.Tanh()))\r\n",
        "        \r\n",
        "        temp_gene = nn.Sequential(*temp_gene)\r\n",
        "\r\n",
        "        self.generators.append(temp_gene)\r\n",
        "\r\n",
        "    def forward(self, z, img = None):\r\n",
        "        ret = []\r\n",
        "        out = None\r\n",
        "        if img != None:\r\n",
        "            out = img\r\n",
        "        else:\r\n",
        "            out = self.generators[0](z[0])\r\n",
        "        ret.append(out)\r\n",
        "        for i in range(1, self.current_scale + 1):\r\n",
        "            out = F.interpolate(out, (self.size_list[i], self.size_list[i]), mode = 'bilinear', align_corners = True)\r\n",
        "            prev = out\r\n",
        "            out = F.pad(out, [5,5,5,5], value = 0)\r\n",
        "            out += z[i]\r\n",
        "            out = self.generators[i](out) + prev\r\n",
        "            ret.append(out)\r\n",
        "            \r\n",
        "        return ret\r\n",
        "\r\n",
        "    def progress(self):\r\n",
        "        self.current_scale += 1\r\n",
        "\r\n",
        "        if self.current_scale % 4 == 0:\r\n",
        "            self.hidden *= 2\r\n",
        "        temp_gene = nn.ModuleList()\r\n",
        "\r\n",
        "        temp_gene.append(nn.Sequential(nn.Conv2d(3, self.hidden, 3, 1),\r\n",
        "                                             nn.BatchNorm2d(self.hidden),\r\n",
        "                                             nn.LeakyReLU(0.2)))\r\n",
        "        for _ in range(3):\r\n",
        "            temp_gene.append(nn.Sequential(nn.Conv2d(self.hidden, self.hidden, 3, 1),\r\n",
        "                                                 nn.BatchNorm2d(self.hidden),\r\n",
        "                                                 nn.LeakyReLU(0.2)))\r\n",
        "        temp_gene.append(nn.Sequential(nn.Conv2d(self.hidden, 3, 3, 1),\r\n",
        "                                             nn.Tanh()))\r\n",
        "        \r\n",
        "        temp_gene = nn.Sequential(*temp_gene)\r\n",
        "        \r\n",
        "\r\n",
        "        if self.current_scale % 4 != 0:\r\n",
        "            # continue start learning from prev generator's parameters\r\n",
        "            temp_gene.load_state_dict(self.generators[-1].state_dict())\r\n",
        "\r\n",
        "        self.generators.append(temp_gene)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VIHvljFcKcGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# models\r\n",
        "scale_factor = 4/3\r\n",
        "min_max_ratio = img_size_max / img_size_min\r\n",
        "num_scale = int(np.round(np.log(min_max_ratio)/np.log(scale_factor)))\r\n",
        "size_list = [int(img_size_min * scale_factor ** i) for i in range(num_scale + 1)]"
      ],
      "outputs": [],
      "metadata": {
        "id": "v0esqCqRJjvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "discriminator = Discriminator()\r\n",
        "generator = Generator(25, num_scale, scale_factor)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size_list : [25, 33, 44, 59, 79, 105, 140, 187, 249]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S0pcb-PKKdU",
        "outputId": "08f3d900-0e86-451d-c2f8-f56cce37f3f4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "torch.cuda.set_device(0)\r\n",
        "discriminator = discriminator.cuda(0)\r\n",
        "generator = generator.cuda(0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bYxkKQAuKkvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# optimizers\r\n",
        "dis_opt = torch.optim.Adam(discriminator.discriminators[0].parameters(), 5e-4, (0.5, 0.999))\r\n",
        "gen_opt = torch.optim.Adam(generator.generators[0].parameters(), 5e-4, (0.5, 0.999))"
      ],
      "outputs": [],
      "metadata": {
        "id": "TxFBsOnaKq1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stage = 0\r\n",
        "if load_model is not None:\r\n",
        "    check_load = open(os.path.join(log_dir, \"checkpoint.txt\"), 'r')\r\n",
        "    to_restore = check_load.readlines()[-1].strip()\r\n",
        "    load_file = os.path.join(log_dir, to_restore)\r\n",
        "    if os.path.isfile(load_file):\r\n",
        "        print(\"=> loading checkpoint '{}'\".format(load_file))\r\n",
        "        checkpoint = torch.load(load_file, map_location='cpu')\r\n",
        "        for _ in range(int(checkpoint['stage'])):\r\n",
        "            generator.progress()\r\n",
        "            discriminator.progress()\r\n",
        "        networks = [discriminator, generator]\r\n",
        "        \r\n",
        "        torch.cuda.set_device(0)\r\n",
        "        networks = [x.cuda(0) for x in networks]\r\n",
        "\r\n",
        "        discriminator, generator, = networks\r\n",
        "        \r\n",
        "        stage = checkpoint['stage']\r\n",
        "        print(\"stage: \",stage)\r\n",
        "        discriminator.load_state_dict(checkpoint['D_state_dict'])\r\n",
        "        generator.load_state_dict(checkpoint['G_state_dict'])\r\n",
        "        dis_opt.load_state_dict(checkpoint['d_optimizer'])\r\n",
        "        gen_opt.load_state_dict(checkpoint['g_optimizer'])\r\n",
        "        print(\"=> loaded checkpoint '{}' (stage {})\"\r\n",
        "              .format(load_file, checkpoint['stage']))\r\n",
        "    else:\r\n",
        "        print(\"=> no checkpoint found at '{}'\".format(log_dir))"
      ],
      "outputs": [],
      "metadata": {
        "id": "xyoHh7ezK2YT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Training\r\n",
        "fixed_latents = [F.pad(torch.randn(batch_size, 3, size_list[0], size_list[0]), [5,5,5,5], value = 0)]\r\n",
        "zero_latents = [F.pad(torch.zeros(batch_size, 3, size_list[idx], size_list[idx]), [5,5,5,5], value = 0) for idx in range(1, num_scale + 1)]\r\n",
        "fixed_latents = fixed_latents + zero_latents"
      ],
      "outputs": [],
      "metadata": {
        "id": "SixT7RQzLMbI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# util functions\r\n",
        "def compute_grad_gp(d_out, x_in):\r\n",
        "    batch_size = x_in.size(0)\r\n",
        "    grad_dout = autograd.grad(\r\n",
        "        outputs=d_out.sum(), inputs=x_in,\r\n",
        "        create_graph=True, retain_graph=True, only_inputs=True)[0]\r\n",
        "    grad_dout2 = grad_dout.pow(2)\r\n",
        "    assert(grad_dout2.size() == x_in.size())\r\n",
        "    reg = grad_dout2.view(batch_size, -1).sum(1)\r\n",
        "    return reg\r\n",
        "\r\n",
        "\r\n",
        "def compute_grad_gp_wgan(D, x_real, x_fake):\r\n",
        "    alpha = torch.rand(x_real.size(0), 1, 1, 1).cuda(0)\r\n",
        "\r\n",
        "    x_interpolate = ((1 - alpha) * x_real + alpha * x_fake).detach()\r\n",
        "    x_interpolate.requires_grad = True\r\n",
        "    d_inter_logit = D(x_interpolate)\r\n",
        "    grad = torch.autograd.grad(d_inter_logit, x_interpolate,\r\n",
        "                               grad_outputs=torch.ones_like(d_inter_logit), create_graph=True)[0]\r\n",
        "\r\n",
        "    norm = grad.view(grad.size(0), -1).norm(p=2, dim=1)\r\n",
        "\r\n",
        "    d_gp = ((norm - 1) ** 2).mean()\r\n",
        "    return d_gp\r\n",
        "\r\n",
        "def save_checkpoint(state, check_list, log_dir, epoch=0):\r\n",
        "    check_file = os.path.join(log_dir, 'model_{}.ckpt'.format(epoch))\r\n",
        "    torch.save(state, check_file)\r\n",
        "    check_list.write('model_{}.ckpt\\n'.format(epoch))\r\n",
        "\r\n",
        "\r\n",
        "class AverageMeter(object):\r\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.val = 0\r\n",
        "        self.avg = 0\r\n",
        "        self.sum = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def update(self, val, n=1):\r\n",
        "        self.val = val\r\n",
        "        self.sum += val * n\r\n",
        "        self.count += n\r\n",
        "        self.avg = self.sum / self.count"
      ],
      "outputs": [],
      "metadata": {
        "id": "FfB8e4YaL8pX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tqdm import trange\r\n",
        "import torchvision.utils as vutils\r\n",
        "\r\n",
        "def train(data_loader, generator, discriminator, d_opt, g_opt, stage_idx, z, size_list, res_dir, gantype, num_scale):\r\n",
        "    generator.train()\r\n",
        "    discriminator.train()\r\n",
        "\r\n",
        "    epochs = 2000\r\n",
        "    decay_lr = 1600\r\n",
        "    train_it = iter(data_loader)\r\n",
        "    origin = train_it.next()\r\n",
        "\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        for z_idx in range(len(z)):\r\n",
        "            z[z_idx] = z[z_idx].cuda(0, non_blocking=True)\r\n",
        "        origin = origin.cuda(0, non_blocking = True)\r\n",
        "    \r\n",
        "    x_in = F.interpolate(origin, (size_list[stage_idx], size_list[stage_idx]), mode = 'bilinear', align_corners = True)\r\n",
        "    vutils.save_image(x_in.detach().cpu(), os.path.join(res_dir, \"ORGINAL_{}.png\".format(stage_idx)), nrow = 1, normalize = True)\r\n",
        "\r\n",
        "    x_in_list = [x_in]\r\n",
        "    for idx in range(1, stage_idx + 1):\r\n",
        "        x_in_list.append(F.interpolate(origin, (size_list[idx], size_list[idx]), mode = 'bilinear', align_corners = True))\r\n",
        "\r\n",
        "    tqdm_train = trange(0, epochs, initial = 0, total = epochs)\r\n",
        "\r\n",
        " \r\n",
        "    d_losses = AverageMeter()\r\n",
        "    g_losses = AverageMeter()\r\n",
        "    for i in tqdm_train:\r\n",
        "        if i == decay_lr:\r\n",
        "            for params in d_opt.param_groups:\r\n",
        "                params['lr'] *= 0.1\r\n",
        "\r\n",
        "            for params in g_opt.param_groups:\r\n",
        "                params['lr'] *= 0.1\r\n",
        "            print(\"Generator and Discriminator's learning rate updated\")\r\n",
        "\r\n",
        "        # update Generator's weights\r\n",
        "        for _ in range(3):\r\n",
        "            g_opt.zero_grad()\r\n",
        "\r\n",
        "            out = generator(z)            \r\n",
        "\r\n",
        "            g_mse = F.mse_loss(out[-1], x_in)\r\n",
        "\r\n",
        "            sqrt_rmse = [1.0]\r\n",
        "            # calc rmse for every scale (except stage 0)\r\n",
        "            for idx in range(1, stage_idx + 1):\r\n",
        "                sqrt_rmse.append(torch.sqrt(F.mse_loss(out[idx], x_in_list[idx])))\r\n",
        "\r\n",
        "            # 각 scale의 sqrt_rmse의 값을 랜덤 값에 곱해준 리스트 생성\r\n",
        "\r\n",
        "            z_list = [F.pad(sqrt_rmse[z_idx] * torch.randn(1, 3, size_list[z_idx],\r\n",
        "                                               size_list[z_idx]).cuda(0, non_blocking=True),\r\n",
        "                            [5, 5, 5, 5], value=0) for z_idx in range(stage_idx + 1)]\r\n",
        "            \r\n",
        "            x_fake_list = generator(z_list)\r\n",
        "            g_fake_logit = discriminator(x_fake_list[-1])\r\n",
        "            if torch.cuda.is_available():\r\n",
        "                ones = torch.ones_like(g_fake_logit).cuda(0)\r\n",
        "            else:\r\n",
        "                ones = torch.ones_like(g_fake_logit)\r\n",
        "\r\n",
        "            if gantype == 'wgangp':\r\n",
        "                # wgan gp\r\n",
        "                g_fake = -torch.mean(g_fake_logit, (2, 3))\r\n",
        "                g_loss = g_fake + 10.0 * g_mse\r\n",
        "            elif gantype == 'zerogp':\r\n",
        "                # zero centered GP\r\n",
        "                g_fake = F.binary_cross_entropy_with_logits(g_fake_logit, ones, reduction='none').mean()\r\n",
        "                g_loss = g_fake + 100.0 * g_mse\r\n",
        "\r\n",
        "            g_loss.backward()\r\n",
        "            g_opt.step()\r\n",
        "            g_losses.update(g_loss.item(), x_in.size(0))\r\n",
        "\r\n",
        "        # Update Discriminator's weights\r\n",
        "        for _ in range(3):\r\n",
        "            x_in.requires_grad = True\r\n",
        "\r\n",
        "            d_opt.zero_grad()\r\n",
        "            x_fake_list = generator(z_list)\r\n",
        "\r\n",
        "            d_fake_logit = discriminator(x_fake_list[-1].detach())\r\n",
        "            d_real_logit = discriminator(x_in)\r\n",
        "\r\n",
        "            if torch.cuda.is_available():\r\n",
        "                ones = torch.ones_like(d_real_logit).cuda(0)\r\n",
        "                zeros = torch.zeros_like(d_fake_logit).cuda(0)\r\n",
        "\r\n",
        "            if gantype == 'wgangp':\r\n",
        "                # wgan gp\r\n",
        "                d_fake = torch.mean(d_fake_logit, (2, 3))\r\n",
        "                d_real = -torch.mean(d_real_logit, (2, 3))\r\n",
        "                d_gp = compute_grad_gp_wgan(discriminator, x_in, x_fake_list[-1])\r\n",
        "                d_loss = d_real + d_fake + 0.1 * d_gp\r\n",
        "\r\n",
        "            elif gantype == 'zerogp':\r\n",
        "                # zero centered GP\r\n",
        "                d_fake = F.binary_cross_entropy_with_logits(d_fake_logit, zeros, reduction='none').mean()\r\n",
        "                d_real = F.binary_cross_entropy_with_logits(d_real_logit, ones, reduction='none').mean()\r\n",
        "                d_gp = compute_grad_gp(torch.mean(d_real_logit, (2, 3)), x_in)\r\n",
        "                d_loss = d_real + d_fake + 10.0 * d_gp\r\n",
        "\r\n",
        "            d_loss.backward()\r\n",
        "            d_opt.step()\r\n",
        "            d_losses.update(d_loss.item(), x_in.size(0))\r\n",
        "\r\n",
        "        tqdm_train.set_description(f'Stage: [{stage_idx}/{num_scale}] Avg Loss: D[{d_losses.avg : .3f}] G[{g_losses.avg : .3f}] RMSE[{sqrt_rmse[-1] : .3f}]')"
      ],
      "outputs": [],
      "metadata": {
        "id": "4YX3bM2HNFSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def validation_func(data_loader, generator, discriminator, stage_idx, z, size_list, res_dir, validation):\r\n",
        "    discriminator.eval()\r\n",
        "    generator.eval()\r\n",
        "\r\n",
        "    val_iter = iter(data_loader)\r\n",
        "    origin = next(val_iter)\r\n",
        "    \r\n",
        "    if torch.cuda.is_available():\r\n",
        "        origin = origin.cuda(0, non_blocking = True)\r\n",
        "    x_in = F.interpolate(origin, (size_list[stage_idx], size_list[stage_idx]), mode='bilinear', align_corners=True)\r\n",
        "    vutils.save_image(x_in.detach().cpu(), os.path.join(res_dir, 'ORG_{}.png'.format(stage_idx)),\r\n",
        "                      nrow=1, normalize=True)\r\n",
        "    x_in_list = [x_in]\r\n",
        "    for xidx in range(1, stage_idx + 1):\r\n",
        "        x_tmp = F.interpolate(origin, (size_list[xidx], size_list[xidx]), mode='bilinear', align_corners=True)\r\n",
        "        x_in_list.append(x_tmp)\r\n",
        "\r\n",
        "    for z_idx in range(len(z)):\r\n",
        "        z[z_idx] = z[z_idx].cuda(0, non_blocking=True)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        out = generator(z)\r\n",
        "\r\n",
        "        # calculate rmse for each scale\r\n",
        "        rmse_list = [1.0]\r\n",
        "        for rmseidx in range(1, stage_idx + 1):\r\n",
        "            rmse = torch.sqrt(F.mse_loss(out[rmseidx], x_in_list[rmseidx]))\r\n",
        "            if validation:\r\n",
        "                rmse /= 100.0\r\n",
        "            rmse_list.append(rmse)\r\n",
        "        if len(rmse_list) > 1:\r\n",
        "            rmse_list[-1] = 0.0\r\n",
        "        if validation:\r\n",
        "            vutils.save_image(out[-1].detach().cpu(), os.path.join(res_dir, 'validation_REC_{}.png'.format(stage_idx)),\r\n",
        "                              nrow=1, normalize=True)\r\n",
        "        else:\r\n",
        "            vutils.save_image(out[-1].detach().cpu(), os.path.join(res_dir, 'REC_{}.png'.format(stage_idx)),\r\n",
        "                              nrow=1, normalize=True)\r\n",
        "\r\n",
        "        for k in range(50):\r\n",
        "            z_list = [F.pad(rmse_list[z_idx] * torch.randn(1, 3, size_list[z_idx],\r\n",
        "                                               size_list[z_idx]).cuda(0, non_blocking=True),\r\n",
        "                            [5, 5, 5, 5], value=0) for z_idx in range(stage_idx + 1)]\r\n",
        "            x_fake_list = generator(z_list)\r\n",
        "            if validation:\r\n",
        "                vutils.save_image(x_fake_list[-1].detach().cpu(), os.path.join(res_dir, 'validation_GEN_{}_{}.png'.format(stage_idx, k)),\r\n",
        "                                  nrow=1, normalize=True)\r\n",
        "            else:\r\n",
        "                vutils.save_image(x_fake_list[-1].detach().cpu(), os.path.join(res_dir, 'GEN_{}_{}.png'.format(stage_idx, k)),\r\n",
        "                                  nrow=1, normalize=True)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "kCYoY4ALN2ZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if validation:\r\n",
        "    validation_func(train_loader, generator, discriminator, stage, fixed_latents, res_dir, validation)\r\n",
        "else:        \r\n",
        "    for stage_idx in range(stage, num_scale + 1):\r\n",
        "        \r\n",
        "        train(train_loader, generator, discriminator, dis_opt, gen_opt, stage_idx, fixed_latents, size_list, res_dir, gantype, num_scale)\r\n",
        "        validation_func(train_loader, generator, discriminator, stage_idx, fixed_latents, size_list, res_dir, validation)\r\n",
        "        discriminator.progress()\r\n",
        "        generator.progress()\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            discriminator = discriminator.cuda(0)\r\n",
        "            generator = generator.cuda(0)\r\n",
        "            \r\n",
        "        # Update the networks at finest scale\r\n",
        "        for net_idx in range(generator.current_scale):\r\n",
        "            for param in generator.generators[net_idx].parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "            for param in discriminator.discriminators[net_idx].parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "\r\n",
        "        dis_opt = torch.optim.Adam(discriminator.discriminators[discriminator.current_scale].parameters(),\r\n",
        "                                    5e-4, (0.5, 0.999))\r\n",
        "        gen_opt = torch.optim.Adam(generator.generators[generator.current_scale].parameters(),\r\n",
        "                                    5e-4, (0.5, 0.999))\r\n",
        "\r\n",
        "\r\n",
        "        if stage_idx == 0:            \r\n",
        "            check_list = open(os.path.join(log_dir, \"checkpoint.txt\"), \"a+\")\r\n",
        "\r\n",
        "        save_checkpoint({\r\n",
        "            'stage': stage_idx + 1,\r\n",
        "            'D_state_dict': discriminator.state_dict(),\r\n",
        "            'G_state_dict': generator.state_dict(),\r\n",
        "            'd_optimizer': dis_opt.state_dict(),\r\n",
        "            'g_optimizer': gen_opt.state_dict()\r\n",
        "        }, check_list, log_dir, stage_idx + 1)\r\n",
        "\r\n",
        "        if stage_idx == num_scale:\r\n",
        "            check_list.close()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Stage: [0/8] Avg Loss: D[ 0.063] G[ 5.536] RMSE[ 1.000]:  80%|████████  | 1602/2000 [01:23<00:20, 19.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [0/8] Avg Loss: D[ 0.055] G[ 5.667] RMSE[ 1.000]: 100%|██████████| 2000/2000 [01:44<00:00, 19.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [1/8] Avg Loss: D[ 0.679] G[ 2.714] RMSE[ 0.025]:  80%|████████  | 1603/2000 [01:38<00:24, 16.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [1/8] Avg Loss: D[ 0.583] G[ 2.946] RMSE[ 0.023]: 100%|██████████| 2000/2000 [02:02<00:00, 16.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [2/8] Avg Loss: D[ 0.702] G[ 2.360] RMSE[ 0.035]:  80%|████████  | 1601/2000 [01:46<00:27, 14.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [2/8] Avg Loss: D[ 0.602] G[ 2.611] RMSE[ 0.042]: 100%|██████████| 2000/2000 [02:13<00:00, 14.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [3/8] Avg Loss: D[ 0.815] G[ 2.086] RMSE[ 0.032]:  80%|████████  | 1601/2000 [01:58<00:28, 14.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [3/8] Avg Loss: D[ 0.728] G[ 2.290] RMSE[ 0.029]: 100%|██████████| 2000/2000 [02:28<00:00, 13.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [4/8] Avg Loss: D[ 0.953] G[ 1.740] RMSE[ 0.037]:  80%|████████  | 1601/2000 [04:03<00:59,  6.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [4/8] Avg Loss: D[ 0.828] G[ 1.981] RMSE[ 0.031]: 100%|██████████| 2000/2000 [05:04<00:00,  6.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [5/8] Avg Loss: D[ 0.992] G[ 1.449] RMSE[ 0.032]:  80%|████████  | 1601/2000 [04:56<01:13,  5.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [5/8] Avg Loss: D[ 0.904] G[ 1.594] RMSE[ 0.024]: 100%|██████████| 2000/2000 [06:10<00:00,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [6/8] Avg Loss: D[ 0.893] G[ 1.483] RMSE[ 0.025]:  80%|████████  | 1600/2000 [12:20<03:04,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [6/8] Avg Loss: D[ 0.821] G[ 1.631] RMSE[ 0.023]: 100%|██████████| 2000/2000 [15:24<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [7/8] Avg Loss: D[ 0.945] G[ 1.293] RMSE[ 0.024]:  80%|████████  | 1600/2000 [15:46<03:56,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [7/8] Avg Loss: D[ 0.888] G[ 1.412] RMSE[ 0.022]: 100%|██████████| 2000/2000 [19:43<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [8/8] Avg Loss: D[ 1.241] G[ 1.257] RMSE[ 0.022]:  80%|████████  | 1600/2000 [33:46<08:28,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator's learning rate updated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage: [8/8] Avg Loss: D[ 1.263] G[ 1.159] RMSE[ 0.021]: 100%|██████████| 2000/2000 [42:13<00:00,  1.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROGRESSION DONE\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ox8wnRjLqiN",
        "outputId": "8208e38d-62eb-4163-8bff-70644e165c1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "a7wQCueZTOLw"
      }
    }
  ]
}